{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.argv = sys.argv[:1]\n",
    "from model import ParserModel, main\n",
    "\n",
    "import time\n",
    "\n",
    "from itertools import islice\n",
    "from sys import stdout\n",
    "from tempfile import NamedTemporaryFile\n",
    "import tensorflow as tf\n",
    "from utils.model import Model\n",
    "from data import load_and_preprocess_data\n",
    "from data import score_arcs\n",
    "from initialization import xavier_weight_init\n",
    "from parser import minibatch_parse\n",
    "from utils.generic_utils import Progbar\n",
    "\n",
    "from tensorflow.python.tools.freeze_graph import freeze_graph\n",
    "import tfcoreml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    n_word_ids = None # inferred\n",
    "    n_tag_ids = None # inferred\n",
    "    n_deprel_ids = None # inferred\n",
    "    n_word_features = None # inferred\n",
    "    n_tag_features = None # inferred\n",
    "    n_deprel_features = None # inferred\n",
    "    n_classes = None # inferred\n",
    "    dropout = 0.5\n",
    "    embed_size = None # inferred\n",
    "    hidden_size = FLAGS.hidden_size \n",
    "    batch_size = 2048\n",
    "    n_epochs = FLAGS.epochs\n",
    "    lr = FLAGS.lr\n",
    "    l2_beta = FLAGS.l2_beta\n",
    "    l2_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING\n",
      "================================================================================\n",
      "Loading word embeddings...there are 4003 word embeddings.\n",
      "Determining POS tags...['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
      "there are 17 tags.\n",
      "Determining deprel labels...['acl', 'acl:relcl', 'advcl', 'advmod', 'amod', 'appos', 'aux', 'auxpass', 'case', 'cc', 'cc:preconj', 'ccomp', 'compound', 'compound:prt', 'conj', 'cop', 'csubj', 'csubjpass', 'dep', 'det', 'det:predet', 'discourse', 'dobj', 'expl', 'iobj', 'mark', 'mwe', 'neg', 'nmod', 'nmod:npmod', 'nmod:poss', 'nmod:tmod', 'nsubj', 'nsubjpass', 'nummod', 'parataxis', 'punct', 'root', 'xcomp']\n",
      "there are 39 deprel labels.\n",
      "Getting training data...there are 1895754 samples.\n",
      "Getting dev data...there are 1700 samples.\n",
      "Getting test data...there are 2416 samples.\n",
      "Word feat size: 18, tag feat size: 18, deprel feat size: 12, classes size: 83\n"
     ]
    }
   ],
   "source": [
    "'''Main function\n",
    "\n",
    "Args:\n",
    "debug :\n",
    "    whether to use a fraction of the data. Make sure to set to False\n",
    "    when you're ready to train your model for real!\n",
    "'''\n",
    "print(80 * \"=\")\n",
    "print(\"INITIALIZING\")\n",
    "print(80 * \"=\")\n",
    "config = Config()\n",
    "data = load_and_preprocess_data(\n",
    "    max_batch_size=config.batch_size)\n",
    "transducer, word_embeddings, train_data = data[:3]\n",
    "dev_sents, dev_arcs = data[3:5]\n",
    "test_sents, test_arcs = data[5:]\n",
    "config.n_word_ids = len(transducer.id2word) + 1 # plus null\n",
    "config.n_tag_ids = len(transducer.id2tag) + 1\n",
    "config.n_deprel_ids = len(transducer.id2deprel) + 1\n",
    "config.embed_size = word_embeddings.shape[1]\n",
    "for (word_batch, tag_batch, deprel_batch), td_batch in \\\n",
    "        train_data.get_iterator(shuffled=False):\n",
    "    config.n_word_features = word_batch.shape[-1]\n",
    "    config.n_tag_features = tag_batch.shape[-1]\n",
    "    config.n_deprel_features = deprel_batch.shape[-1]\n",
    "    config.n_classes = td_batch.shape[-1]\n",
    "    break\n",
    "print(\n",
    "    'Word feat size: {}, tag feat size: {}, deprel feat size: {}, '\n",
    "    'classes size: {}'.format(\n",
    "        config.n_word_features, config.n_tag_features,\n",
    "        config.n_deprel_features, config.n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model... \n",
      "\trelu activation function\n",
      "\t1 hidden layer(s) with size 200\n",
      "\tadam optimizer with learning rate 0.001\n",
      "took 1.55 seconds\n",
      "\n",
      "INFO:tensorflow:Froze 9 variables.\n",
      "INFO:tensorflow:Converted 9 variables to const ops.\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 0\n",
      "1895754/1895754 [==============================] - 51s 27us/step - Cross-entropy: 0.3934\n",
      "[('Influential', 'ADJ'), ('members', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('House', 'PROPN'), ('Ways', 'PROPN'), ('and', 'CONJ'), ('Means', 'PROPN'), ('Committee', 'PROPN'), ('introduced', 'VERB'), ('legislation', 'NOUN'), ('that', 'PRON'), ('would', 'AUX'), ('restrict', 'VERB'), ('how', 'ADV'), ('the', 'DET'), ('new', 'ADJ'), ('savings-and-loan', 'NOUN'), ('bailout', 'NOUN'), ('agency', 'NOUN'), ('can', 'AUX'), ('raise', 'VERB'), ('capital', 'NOUN'), (',', 'PUNCT'), ('creating', 'VERB'), ('another', 'DET'), ('potential', 'ADJ'), ('obstacle', 'NOUN'), ('to', 'ADP'), ('the', 'DET'), ('government', 'NOUN'), (\"'s\", 'PART'), ('sale', 'NOUN'), ('of', 'ADP'), ('sick', 'ADJ'), ('thrifts', 'NOUN'), ('.', 'PUNCT')]\n",
      "INFO:tensorflow:Froze 9 variables.\n",
      "INFO:tensorflow:Converted 9 variables to const ops.\n",
      "Validation LAS: 0.82 (BEST!), \n",
      "Validation UAS: 0.84\n",
      "\n",
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "[('No', 'ADV'), (',', 'PUNCT'), ('it', 'PRON'), ('was', 'VERB'), (\"n't\", 'PART'), ('Black', 'PROPN'), ('Monday', 'PROPN'), ('.', 'PUNCT')]\n",
      "Test LAS: 0.83, Test UAS: 0.85\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if debug:\n",
    "    dev_sents = dev_sents[:500]\n",
    "    dev_arcs = dev_arcs[:500]\n",
    "    test_sents = test_sents[:500]\n",
    "    test_arcs = test_arcs[:500]\n",
    "if not debug:\n",
    "    weight_file = NamedTemporaryFile(suffix='.weights')\n",
    "    # weight_file = open(\"something.weights\", mode=)\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    print(\"Building model...\", end=' ')\n",
    "    start = time.time()\n",
    "    model = ParserModel(transducer, session, config, word_embeddings, is_training=True)\n",
    "    print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    output_names = 'output/td_vec'\n",
    "    saver = None if debug else tf.train.Saver()\n",
    "#     saver.restore(session, \"checkpoints/model.ckpt\")\n",
    "    frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "        sess=session,\n",
    "        input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),\n",
    "        output_node_names=[output_names])\n",
    "    frozen_graph = tf.compat.v1.graph_util.extract_sub_graph(\n",
    "        graph_def=frozen_graph,\n",
    "        dest_nodes=[output_names])\n",
    "    with open('checkpoints/frozen_graph.pb', 'wb') as fout:\n",
    "        fout.write(frozen_graph.SerializeToString())\n",
    "    print(80 * \"=\")\n",
    "    print(\"TRAINING\")\n",
    "    print(80 * \"=\")\n",
    "    best_las = 0.\n",
    "    for epoch in range(config.n_epochs):\n",
    "        print('Epoch {}'.format(epoch))\n",
    "\n",
    "        if debug:\n",
    "            model.fit_epoch(list(islice(train_data,3)), config.batch_size)\n",
    "        else:\n",
    "            model.fit_epoch(train_data)\n",
    "        stdout.flush()\n",
    "        dev_las, dev_uas = model.eval(dev_sents, dev_arcs)\n",
    "        best = dev_las > best_las\n",
    "        if best:\n",
    "            best_las = dev_las\n",
    "            if not debug:\n",
    "                saver.save(session, \"checkpoints/model.ckpt\")\n",
    "                tf.io.write_graph(session.graph_def, './checkpoints/', 'model.pbtxt')\n",
    "                frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "                    sess=session,\n",
    "                    input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),\n",
    "                    output_node_names=[output_names])\n",
    "                frozen_graph = tf.compat.v1.graph_util.extract_sub_graph(\n",
    "                    graph_def=frozen_graph,\n",
    "                    dest_nodes=[output_names])\n",
    "                with open('checkpoints/frozen_graph.pb', 'wb') as fout:\n",
    "                    fout.write(frozen_graph.SerializeToString())\n",
    "\n",
    "        print('Validation LAS: ', end='')\n",
    "        print('{:.2f}{}'.format(dev_las, ' (BEST!), ' if best else ', '))\n",
    "        print('Validation UAS: ', end='')\n",
    "        print('{:.2f}'.format(dev_uas))\n",
    "    if not debug:\n",
    "        print()\n",
    "        print(80 * \"=\")\n",
    "        print(\"TESTING\")\n",
    "        print(80 * \"=\")\n",
    "        print(\"Restoring the best model weights found on the dev set\")\n",
    "        saver.restore(session, \"checkpoints/model.ckpt\")\n",
    "        stdout.flush()\n",
    "        las,uas = model.eval(test_sents, test_arcs)\n",
    "        if las:\n",
    "            print(\"Test LAS: \", end='')\n",
    "            print('{:.2f}'.format(las), end=', ')\n",
    "        print(\"Test UAS: \", end='')\n",
    "        print('{:.2f}'.format(uas))\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model... \n",
      "\trelu activation function\n",
      "\t1 hidden layer(s) with size 200\n",
      "\tadam optimizer with learning rate 0.001\n",
      "took 0.31 seconds\n",
      "\n",
      "INFO:tensorflow:Froze 9 variables.\n",
      "INFO:tensorflow:Converted 9 variables to const ops.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "[[('No', 'ADV'), (',', 'PUNCT'), ('it', 'PRON'), ('was', 'VERB'), (\"n't\", 'PART'), ('Black', 'PROPN'), ('Monday', 'PROPN'), ('.', 'PUNCT')], [('But', 'CONJ'), ('while', 'SCONJ'), ('the', 'DET'), ('New', 'PROPN'), ('York', 'PROPN'), ('Stock', 'PROPN'), ('Exchange', 'PROPN'), ('did', 'AUX'), (\"n't\", 'PART'), ('fall', 'VERB'), ('apart', 'ADV'), ('Friday', 'PROPN'), ('as', 'SCONJ'), ('the', 'DET'), ('Dow', 'PROPN'), ('Jones', 'PROPN'), ('Industrial', 'PROPN'), ('Average', 'PROPN'), ('plunged', 'VERB'), ('190.58', 'NUM'), ('points', 'NOUN'), ('--', 'PUNCT'), ('most', 'ADJ'), ('of', 'ADP'), ('it', 'PRON'), ('in', 'ADP'), ('the', 'DET'), ('final', 'ADJ'), ('hour', 'NOUN'), ('--', 'PUNCT'), ('it', 'PRON'), ('barely', 'ADV'), ('managed', 'VERB'), ('to', 'PART'), ('stay', 'VERB'), ('this', 'DET'), ('side', 'NOUN'), ('of', 'ADP'), ('chaos', 'NOUN'), ('.', 'PUNCT')], ...]\n",
      "[('No', 'ADV'), (',', 'PUNCT'), ('it', 'PRON'), ('was', 'VERB'), (\"n't\", 'PART'), ('Black', 'PROPN'), ('Monday', 'PROPN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    print(\"Building model...\", end=' ')\n",
    "    start = time.time()\n",
    "    model = ParserModel(transducer, session, config, word_embeddings, is_training=True)\n",
    "    print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    output_names = 'output/td_vec'\n",
    "    saver = None if debug else tf.train.Saver()\n",
    "#     saver.restore(session, \"checkpoints/model.ckpt\")\n",
    "    frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "        sess=session,\n",
    "        input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),\n",
    "        output_node_names=[output_names])\n",
    "    frozen_graph = tf.compat.v1.graph_util.extract_sub_graph(\n",
    "        graph_def=frozen_graph,\n",
    "        dest_nodes=[output_names])\n",
    "    with open('checkpoints/frozen_graph.pb', 'wb') as fout:\n",
    "        fout.write(frozen_graph.SerializeToString())\n",
    "    saver.restore(session, \"checkpoints/model.ckpt\")\n",
    "    print(test_sents)\n",
    "    las,uas = model.eval(test_sents, test_arcs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0., 4002., 4002., ..., 4002., 4002., 4002.],\n",
       "        [ 593.,    0., 4002., ..., 4002., 4002., 4002.],\n",
       "        [1237.,  593.,    0., ..., 4002., 4002., 4002.],\n",
       "        ...,\n",
       "        [ 909., 3706., 3957., ..., 4002., 4002., 4002.],\n",
       "        [ 909., 3957., 3162., ..., 4002., 4002., 4002.],\n",
       "        [ 909., 3162., 1200., ..., 4002., 4002., 4002.]], dtype=float32),\n",
       " array([[ 0., 19., 19., ..., 19., 19., 19.],\n",
       "        [ 2.,  0., 19., ..., 19., 19., 19.],\n",
       "        [ 6.,  2.,  0., ..., 19., 19., 19.],\n",
       "        ...,\n",
       "        [12.,  6.,  2., ..., 19., 19., 19.],\n",
       "        [12.,  2., 16., ..., 19., 19., 19.],\n",
       "        [12., 16.,  8., ..., 19., 19., 19.]], dtype=float32),\n",
       " array([[41., 41., 41., ..., 41., 41., 41.],\n",
       "        [41., 41., 41., ..., 41., 41., 41.],\n",
       "        [41., 41., 41., ..., 41., 41., 41.],\n",
       "        ...,\n",
       "        [41., 41., 41., ..., 41., 41., 41.],\n",
       "        [20., 41., 41., ..., 41., 41., 41.],\n",
       "        [ 9., 20., 23., ..., 41., 41., 41.]], dtype=float32))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transducer, word_embeddings, train_data = data[:3]\n",
    "# dev_sents, dev_arcs = data[3:5]\n",
    "# test_sents, test_arcs = data[5:]\n",
    "# print(dev_sents[:1])\n",
    "# print(dev_arcs[:1])\n",
    "import json\n",
    "list(islice(train_data,1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
