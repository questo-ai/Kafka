{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow version 1.15.2 detected. Last version known to be fully compatible is 1.14.0 .\n",
      "WARNING:root:Keras version 2.3.1 detected. Last version known to be fully compatible of Keras is 2.2.4 .\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.argv = sys.argv[:1]\n",
    "from model import ParserModel, main, Config\n",
    "\n",
    "import time\n",
    "\n",
    "from itertools import islice\n",
    "from sys import stdout\n",
    "from tempfile import NamedTemporaryFile\n",
    "import tensorflow as tf\n",
    "from utils.model import Model\n",
    "from data import load_and_preprocess_data\n",
    "from data import score_arcs\n",
    "from initialization import xavier_weight_init\n",
    "from parser import minibatch_parse\n",
    "from utils.generic_utils import Progbar\n",
    "\n",
    "from tensorflow.python.tools.freeze_graph import freeze_graph\n",
    "import tfcoreml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Config(object):\n",
    "#     \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "#     The config class is used to store various hyperparameters and dataset\n",
    "#     information parameters. Model objects are passed a Config() object at\n",
    "#     instantiation.\n",
    "#     \"\"\"\n",
    "#     n_word_ids = None # inferred\n",
    "#     n_tag_ids = None # inferred\n",
    "#     n_deprel_ids = None # inferred\n",
    "#     n_word_features = None # inferred\n",
    "#     n_tag_features = None # inferred\n",
    "#     n_deprel_features = None # inferred\n",
    "#     n_classes = None # inferred\n",
    "#     dropout = 0.5\n",
    "#     embed_size = None # inferred\n",
    "#     hidden_size = 4\n",
    "#     batch_size = 2048\n",
    "#     n_epochs = 1\n",
    "#     lr = 0.001\n",
    "#     l2_beta = 10e-8\n",
    "#     l2_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING\n",
      "================================================================================\n",
      "Loading word embeddings...there are 4003 word embeddings.\n",
      "Determining POS tags...['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
      "there are 17 tags.\n",
      "Determining deprel labels...['acl', 'acl:relcl', 'advcl', 'advmod', 'amod', 'appos', 'aux', 'auxpass', 'case', 'cc', 'cc:preconj', 'ccomp', 'compound', 'compound:prt', 'conj', 'cop', 'csubj', 'csubjpass', 'dep', 'det', 'det:predet', 'discourse', 'dobj', 'expl', 'iobj', 'mark', 'mwe', 'neg', 'nmod', 'nmod:npmod', 'nmod:poss', 'nmod:tmod', 'nsubj', 'nsubjpass', 'nummod', 'parataxis', 'punct', 'root', 'xcomp']\n",
      "there are 39 deprel labels.\n",
      "Getting training data...there are 1895754 samples.\n",
      "Getting dev data...there are 1700 samples.\n",
      "Getting test data...there are 2416 samples.\n",
      "Word feat size: 18, tag feat size: 18, deprel feat size: 12, classes size: 83\n"
     ]
    }
   ],
   "source": [
    "'''Main function\n",
    "\n",
    "Args:\n",
    "debug :\n",
    "    whether to use a fraction of the data. Make sure to set to False\n",
    "    when you're ready to train your model for real!\n",
    "'''\n",
    "print(80 * \"=\")\n",
    "print(\"INITIALIZING\")\n",
    "print(80 * \"=\")\n",
    "config = Config()\n",
    "data = load_and_preprocess_data(\n",
    "    max_batch_size=config.batch_size)\n",
    "transducer, word_embeddings, train_data = data[:3]\n",
    "dev_sents, dev_arcs = data[3:5]\n",
    "test_sents, test_arcs = data[5:]\n",
    "config.n_word_ids = len(transducer.id2word) + 1 # plus null\n",
    "config.n_tag_ids = len(transducer.id2tag) + 1\n",
    "config.n_deprel_ids = len(transducer.id2deprel) + 1\n",
    "config.embed_size = word_embeddings.shape[1]\n",
    "for (word_batch, tag_batch, deprel_batch), td_batch in \\\n",
    "        train_data.get_iterator(shuffled=False):\n",
    "    config.n_word_features = word_batch.shape[-1]\n",
    "    config.n_tag_features = tag_batch.shape[-1]\n",
    "    config.n_deprel_features = deprel_batch.shape[-1]\n",
    "    config.n_classes = td_batch.shape[-1]\n",
    "    break\n",
    "print(\n",
    "    'Word feat size: {}, tag feat size: {}, deprel feat size: {}, '\n",
    "    'classes size: {}'.format(\n",
    "        config.n_word_features, config.n_tag_features,\n",
    "        config.n_deprel_features, config.n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DON'T RERUN THINGS BEFORE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model... WARNING:tensorflow:From /Users/khushjammu/GitHub/spacy-ios/neural-dependency-parser/model.py:97: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/khushjammu/GitHub/spacy-ios/neural-dependency-parser/model.py:97: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/khushjammu/GitHub/spacy-ios/neural-dependency-parser/initialization.py:15: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/khushjammu/GitHub/spacy-ios/neural-dependency-parser/initialization.py:15: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\trelu activation function\n",
      "\tl2 regularization with beta 1e-07\n",
      "\t5 hidden layer(s) with size 500\n",
      "took 0.76 seconds\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "if debug:\n",
    "    dev_sents = dev_sents[:500]\n",
    "    dev_arcs = dev_arcs[:500]\n",
    "    test_sents = test_sents[:500]\n",
    "    test_arcs = test_arcs[:500]\n",
    "if not debug:\n",
    "    weight_file = NamedTemporaryFile(suffix='.weights')\n",
    "    # weight_file = open(\"something.weights\", mode=)\n",
    "\n",
    "session = tf.Session()\n",
    "print(\"Building model...\", end=' ')\n",
    "start = time.time()\n",
    "model = ParserModel(transducer, session, config, word_embeddings, is_training=False)\n",
    "print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "output_names = 'output/td_vec'\n",
    "saver = None if debug else tf.train.Saver()\n",
    "saver.restore(session, \"checkpoints/model.ckpt\")\n",
    "\n",
    "# frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "#     sess=session,\n",
    "#     input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),\n",
    "#     output_node_names=[output_names])\n",
    "# frozen_graph = tf.compat.v1.graph_util.extract_sub_graph(\n",
    "#     graph_def=frozen_graph,\n",
    "#     dest_nodes=[output_names])\n",
    "# with open('checkpoints/frozen_graph.pb', 'wb') as fout:\n",
    "#     fout.write(frozen_graph.SerializeToString())\n",
    "# print(80 * \"=\")\n",
    "# print(\"TRAINING\")\n",
    "# print(80 * \"=\")\n",
    "# best_las = 0.\n",
    "\n",
    "#     for epoch in range(config.n_epochs):\n",
    "#         print('Epoch {}'.format(epoch))\n",
    "\n",
    "#         if debug:\n",
    "#             model.fit_epoch(list(islice(train_data,3)), config.batch_size)\n",
    "#         else:\n",
    "#             model.fit_epoch(train_data)\n",
    "#         stdout.flush()\n",
    "#         dev_las, dev_uas = model.eval(dev_sents, dev_arcs)\n",
    "#         best = dev_las > best_las\n",
    "#         if best:\n",
    "#             best_las = dev_las\n",
    "#             if not debug:\n",
    "#                 saver.save(session, \"checkpoints/model.ckpt\")\n",
    "#                 tf.io.write_graph(session.graph_def, './checkpoints/', 'model.pbtxt')\n",
    "#                 frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "#                     sess=session,\n",
    "#                     input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),\n",
    "#                     output_node_names=[output_names])\n",
    "#                 frozen_graph = tf.compat.v1.graph_util.extract_sub_graph(\n",
    "#                     graph_def=frozen_graph,\n",
    "#                     dest_nodes=[output_names])\n",
    "#                 with open('checkpoints/frozen_graph.pb', 'wb') as fout:\n",
    "#                     fout.write(frozen_graph.SerializeToString())\n",
    "\n",
    "#         print('Validation LAS: ', end='')\n",
    "#         print('{:.2f}{}'.format(dev_las, ' (BEST!), ' if best else ', '))\n",
    "#         print('Validation UAS: ', end='')\n",
    "#         print('{:.2f}'.format(dev_uas))\n",
    "\n",
    "# if not debug:\n",
    "#     print()\n",
    "#     print(80 * \"=\")\n",
    "#     print(\"TESTING\")\n",
    "#     print(80 * \"=\")\n",
    "#     print(\"Restoring the best model weights found on the dev set\")\n",
    "#     saver.restore(session, \"checkpoints/model.ckpt\")\n",
    "#     stdout.flush()\n",
    "#     las,uas = model.eval(test_sents, test_arcs)\n",
    "#     if las:\n",
    "#         print(\"Test LAS: \", end='')\n",
    "#         print('{:.2f}'.format(las), end=', ')\n",
    "#     print(\"Test UAS: \", end='')\n",
    "#     print('{:.2f}'.format(uas))\n",
    "#     print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Graph().as_default(), tf.Session() as session:\n",
    "#     print(\"Building model...\", end=' ')\n",
    "#     start = time.time()\n",
    "#     model = ParserModel(transducer, session, config, word_embeddings, is_training=True)\n",
    "#     print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     session.run(init)\n",
    "#     output_names = 'output/td_vec'\n",
    "#     saver = None if debug else tf.train.Saver()\n",
    "# #     saver.restore(session, \"checkpoints/model.ckpt\")\n",
    "#     frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "#         sess=session,\n",
    "#         input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),\n",
    "#         output_node_names=[output_names])\n",
    "#     frozen_graph = tf.compat.v1.graph_util.extract_sub_graph(\n",
    "#         graph_def=frozen_graph,\n",
    "#         dest_nodes=[output_names])\n",
    "#     with open('checkpoints/frozen_graph.pb', 'wb') as fout:\n",
    "#         fout.write(frozen_graph.SerializeToString())\n",
    "#     saver.restore(session, \"checkpoints/model.ckpt\")\n",
    "#     print(test_sents)\n",
    "#     las,uas = model.eval(test_sents, test_arcs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transducer, word_embeddings, train_data = data[:3]\n",
    "# dev_sents, dev_arcs = data[3:5]\n",
    "# test_sents, test_arcs = data[5:]\n",
    "# print(dev_sents[:1])\n",
    "# print(dev_arcs[:1])\n",
    "# import json\n",
    "# list(islice(train_data,1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gameplan: run minibatch_parse on single sentence pair, extract the feed. understand structure of said feed. try feeding feed into model directly. if it works => mad profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'parser' from '/Users/khushjammu/GitHub/spacy-ios/neural-dependency-parser/parser.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import parser\n",
    "importlib.reload(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('What', 'PRON'), ('if', 'SCONJ'), ('Google', 'PROPN'), ('Morphed', 'VERB'), ('Into', 'ADP'), ('GoogleOS', 'PROPN'), ('?', 'PUNCT')]\n",
      "FUCK\n",
      "minibatch\n",
      "[<parser.PartialParse object at 0x146a959d0>]\n",
      "feed\n",
      "{<tf.Tensor 'Placeholder:0' shape=(?, 18) dtype=int32>: (array([   0, 4002, 4002, 1087, 2360, 4001, 4002, 4002, 4002, 4002, 4002,\n",
      "       4002, 4002, 4002, 4002, 4002, 4002, 4002], dtype=int32),), <tf.Tensor 'Placeholder_1:0' shape=(?, 18) dtype=int32>: (array([ 0, 19, 19, 11, 14, 12, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "       19], dtype=int32),), <tf.Tensor 'Placeholder_2:0' shape=(?, 12) dtype=int32>: (array([41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41], dtype=int32),), <tf.Tensor 'Placeholder_4:0' shape=() dtype=float32>: 1}\n",
      "td_pairs\n",
      "[(2, None)]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushjammu/opt/anaconda3/envs/port_spacy/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser.minibatch_parse(\n",
    "    [[(\"What\", \"PRON\"), (\"if\", \"SCONJ\"), (\"Google\", \"PROPN\"), (\"Morphed\", \"VERB\"), (\"Into\", \"ADP\"), (\"GoogleOS\", \"PROPN\"), (\"?\", \"PUNCT\")]],\n",
    "    model,\n",
    "    1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (port_spacy)",
   "language": "python",
   "name": "port_spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
