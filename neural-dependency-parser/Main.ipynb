{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow version 1.15.2 detected. Last version known to be fully compatible is 1.14.0 .\n",
      "WARNING:root:Keras version 2.3.1 detected. Last version known to be fully compatible of Keras is 2.2.4 .\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.argv = sys.argv[:1]\n",
    "from model import ParserModel, main, Config\n",
    "\n",
    "import time\n",
    "\n",
    "from itertools import islice\n",
    "from sys import stdout\n",
    "from tempfile import NamedTemporaryFile\n",
    "import tensorflow as tf\n",
    "from utils.model import Model\n",
    "from data import load_and_preprocess_data\n",
    "from data import score_arcs\n",
    "from initialization import xavier_weight_init\n",
    "from parser import minibatch_parse\n",
    "from utils.generic_utils import Progbar\n",
    "\n",
    "from tensorflow.python.tools.freeze_graph import freeze_graph\n",
    "import tfcoreml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Config(object):\n",
    "#     \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "#     The config class is used to store various hyperparameters and dataset\n",
    "#     information parameters. Model objects are passed a Config() object at\n",
    "#     instantiation.\n",
    "#     \"\"\"\n",
    "#     n_word_ids = None # inferred\n",
    "#     n_tag_ids = None # inferred\n",
    "#     n_deprel_ids = None # inferred\n",
    "#     n_word_features = None # inferred\n",
    "#     n_tag_features = None # inferred\n",
    "#     n_deprel_features = None # inferred\n",
    "#     n_classes = None # inferred\n",
    "#     dropout = 0.5\n",
    "#     embed_size = None # inferred\n",
    "#     hidden_size = 4\n",
    "#     batch_size = 2048\n",
    "#     n_epochs = 1\n",
    "#     lr = 0.001\n",
    "#     l2_beta = 10e-8\n",
    "#     l2_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Main function\n",
    "\n",
    "Args:\n",
    "debug :\n",
    "    whether to use a fraction of the data. Make sure to set to False\n",
    "    when you're ready to train your model for real!\n",
    "'''\n",
    "print(80 * \"=\")\n",
    "print(\"INITIALIZING\")\n",
    "print(80 * \"=\")\n",
    "config = Config()\n",
    "data = load_and_preprocess_data(\n",
    "    max_batch_size=config.batch_size)\n",
    "transducer, word_embeddings, train_data = data[:3]\n",
    "dev_sents, dev_arcs = data[3:5]\n",
    "test_sents, test_arcs = data[5:]\n",
    "config.n_word_ids = len(transducer.id2word) + 1 # plus null\n",
    "config.n_tag_ids = len(transducer.id2tag) + 1\n",
    "config.n_deprel_ids = len(transducer.id2deprel) + 1\n",
    "config.embed_size = word_embeddings.shape[1]\n",
    "for (word_batch, tag_batch, deprel_batch), td_batch in \\\n",
    "        train_data.get_iterator(shuffled=False):\n",
    "    config.n_word_features = word_batch.shape[-1]\n",
    "    config.n_tag_features = tag_batch.shape[-1]\n",
    "    config.n_deprel_features = deprel_batch.shape[-1]\n",
    "    config.n_classes = td_batch.shape[-1]\n",
    "    break\n",
    "print(\n",
    "    'Word feat size: {}, tag feat size: {}, deprel feat size: {}, '\n",
    "    'classes size: {}'.format(\n",
    "        config.n_word_features, config.n_tag_features,\n",
    "        config.n_deprel_features, config.n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DON'T RERUN THINGS BEFORE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    dev_sents = dev_sents[:500]\n",
    "    dev_arcs = dev_arcs[:500]\n",
    "    test_sents = test_sents[:500]\n",
    "    test_arcs = test_arcs[:500]\n",
    "if not debug:\n",
    "    weight_file = NamedTemporaryFile(suffix='.weights')\n",
    "    # weight_file = open(\"something.weights\", mode=)\n",
    "\n",
    "session = tf.Session()\n",
    "print(\"Building model...\", end=' ')\n",
    "start = time.time()\n",
    "model = ParserModel(transducer, session, config, word_embeddings, is_training=False)\n",
    "print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "output_names = 'output/td_vec'\n",
    "saver = None if debug else tf.train.Saver()\n",
    "saver.restore(session, \"checkpoints/model.ckpt\")\n",
    "\n",
    "# frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "#     sess=session,\n",
    "#     input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),\n",
    "#     output_node_names=[output_names])\n",
    "# frozen_graph = tf.compat.v1.graph_util.extract_sub_graph(\n",
    "#     graph_def=frozen_graph,\n",
    "#     dest_nodes=[output_names])\n",
    "# with open('checkpoints/frozen_graph.pb', 'wb') as fout:\n",
    "#     fout.write(frozen_graph.SerializeToString())\n",
    "# print(80 * \"=\")\n",
    "# print(\"TRAINING\")\n",
    "# print(80 * \"=\")\n",
    "# best_las = 0.\n",
    "\n",
    "#     for epoch in range(config.n_epochs):\n",
    "#         print('Epoch {}'.format(epoch))\n",
    "\n",
    "#         if debug:\n",
    "#             model.fit_epoch(list(islice(train_data,3)), config.batch_size)\n",
    "#         else:\n",
    "#             model.fit_epoch(train_data)\n",
    "#         stdout.flush()\n",
    "#         dev_las, dev_uas = model.eval(dev_sents, dev_arcs)\n",
    "#         best = dev_las > best_las\n",
    "#         if best:\n",
    "#             best_las = dev_las\n",
    "#             if not debug:\n",
    "#                 saver.save(session, \"checkpoints/model.ckpt\")\n",
    "#                 tf.io.write_graph(session.graph_def, './checkpoints/', 'model.pbtxt')\n",
    "#                 frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "#                     sess=session,\n",
    "#                     input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),\n",
    "#                     output_node_names=[output_names])\n",
    "#                 frozen_graph = tf.compat.v1.graph_util.extract_sub_graph(\n",
    "#                     graph_def=frozen_graph,\n",
    "#                     dest_nodes=[output_names])\n",
    "#                 with open('checkpoints/frozen_graph.pb', 'wb') as fout:\n",
    "#                     fout.write(frozen_graph.SerializeToString())\n",
    "\n",
    "#         print('Validation LAS: ', end='')\n",
    "#         print('{:.2f}{}'.format(dev_las, ' (BEST!), ' if best else ', '))\n",
    "#         print('Validation UAS: ', end='')\n",
    "#         print('{:.2f}'.format(dev_uas))\n",
    "\n",
    "# if not debug:\n",
    "#     print()\n",
    "#     print(80 * \"=\")\n",
    "#     print(\"TESTING\")\n",
    "#     print(80 * \"=\")\n",
    "#     print(\"Restoring the best model weights found on the dev set\")\n",
    "#     saver.restore(session, \"checkpoints/model.ckpt\")\n",
    "#     stdout.flush()\n",
    "#     las,uas = model.eval(test_sents, test_arcs)\n",
    "#     if las:\n",
    "#         print(\"Test LAS: \", end='')\n",
    "#         print('{:.2f}'.format(las), end=', ')\n",
    "#     print(\"Test UAS: \", end='')\n",
    "#     print('{:.2f}'.format(uas))\n",
    "#     print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Graph().as_default(), tf.Session() as session:\n",
    "#     print(\"Building model...\", end=' ')\n",
    "#     start = time.time()\n",
    "#     model = ParserModel(transducer, session, config, word_embeddings, is_training=True)\n",
    "#     print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     session.run(init)\n",
    "#     output_names = 'output/td_vec'\n",
    "#     saver = None if debug else tf.train.Saver()\n",
    "# #     saver.restore(session, \"checkpoints/model.ckpt\")\n",
    "#     frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "#         sess=session,\n",
    "#         input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),\n",
    "#         output_node_names=[output_names])\n",
    "#     frozen_graph = tf.compat.v1.graph_util.extract_sub_graph(\n",
    "#         graph_def=frozen_graph,\n",
    "#         dest_nodes=[output_names])\n",
    "#     with open('checkpoints/frozen_graph.pb', 'wb') as fout:\n",
    "#         fout.write(frozen_graph.SerializeToString())\n",
    "#     saver.restore(session, \"checkpoints/model.ckpt\")\n",
    "#     print(test_sents)\n",
    "#     las,uas = model.eval(test_sents, test_arcs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transducer, word_embeddings, train_data = data[:3]\n",
    "# dev_sents, dev_arcs = data[3:5]\n",
    "# test_sents, test_arcs = data[5:]\n",
    "# print(dev_sents[:1])\n",
    "# print(dev_arcs[:1])\n",
    "# import json\n",
    "# list(islice(train_data,1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gameplan: run minibatch_parse on single sentence pair, extract the feed. understand structure of said feed. try feeding feed into model directly. if it works => mad profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'parser' from '/Users/khushjammu/GitHub/spacy-ios/neural-dependency-parser/parser.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import parser\n",
    "importlib.reload(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.minibatch_parse(\n",
    "    [[(\"What\", \"PRON\"), (\"if\", \"SCONJ\"), (\"Google\", \"PROPN\"), (\"Morphed\", \"VERB\"), (\"Into\", \"ADP\"), (\"GoogleOS\", \"PROPN\"), (\"?\", \"PUNCT\")]],\n",
    "    model,\n",
    "    1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gameplan: run `load_and_preprocess_data` to create `TrainingIterable`-s, which calls `graphs2feats_and_tds` in its constructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data' from '/Users/khushjammu/GitHub/spacy-ios/neural-dependency-parser/data.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data\n",
    "import importlib\n",
    "importlib.reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.load_and_preprocess_data(\n",
    "    max_batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs are fed into the whole pipeline pre-made through `TrainIterable` in `data.py`, via the variable `training_graphs`. \n",
    "\n",
    "#### prediction pathway\n",
    "use minibatch_parse -> instatiates a bunch of partial-parses with sentences -> minibatch of PartialParses created -> model predicts on minibatch to generate td_vecs -> td vecs are used with parse_step to construct the parsed sentence in PartialParse.\n",
    "\n",
    "we have to port transducer (none of the graph bits -- I THINK just pps2feats and td_vec2trans_deprel) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (port_spacy)",
   "language": "python",
   "name": "port_spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
